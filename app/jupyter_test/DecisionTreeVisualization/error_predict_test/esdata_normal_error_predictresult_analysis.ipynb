{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "import os       \n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "os.chdir('D:/zxdf/Workspaces/PycharmProjects/flask_ml')\n",
    "from app import config, Elasticsearch_Util\n",
    "#from app.lib import nltk_text_analyze_lib as textAnalyzer\n",
    "#from app.lib import sklearn_classification_lib as skclassification\n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "print (\"当前的日期和时间是 %s\" % starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMoreDataFromES(self, index, doc_type,beginTime, endTime, source_include,size):\n",
    "    querybody = {\n",
    "        \"query\": {\n",
    "            \"match_all\": {}\n",
    "        },\n",
    "        \"_source\": {\n",
    "            \"includes\": source_include,\n",
    "            \"excludes\": []\n",
    "        }\n",
    "    }\n",
    "    # Initialize the scroll\n",
    "    print \"Initialize the scroll\"\n",
    "    page = self.es.search(\n",
    "        index=index,\n",
    "        doc_type=doc_type,\n",
    "        scroll='2m',\n",
    "        size=size,\n",
    "        body=querybody)\n",
    "    sid = page['_scroll_id']\n",
    "    scroll_size = page['hits']['total']\n",
    "    \n",
    "    print \"sid: \" + str(sid)\n",
    "    print \"scroll size: \" + str(scroll_size)\n",
    "    \n",
    "    records = []\n",
    "    for doc in page['hits']['hits']:\n",
    "            source=doc['_source']\n",
    "            source[\"id\"]=doc['_id']\n",
    "            records.append(source)\n",
    "    \n",
    "    df = None\n",
    "   #Start scrolling\n",
    "    while (scroll_size > 0):\n",
    "        print \"Scrolling...\"\n",
    "        page = self.es.scroll(scroll_id=sid, scroll='2m')\n",
    "        # Update the scroll ID\n",
    "        sid = page['_scroll_id']\n",
    "        print \"sid: \" + str(sid)\n",
    "        # Get the number of results that we returned in the last scroll\n",
    "        scroll_size = len(page['hits']['hits'])\n",
    "        \n",
    "        for doc in page['hits']['hits']:\n",
    "            source=doc['_source']\n",
    "            source[\"id\"]=doc['_id']\n",
    "            records.append(source)\n",
    "            \n",
    "        print \"scroll size: \" + str(scroll_size)\n",
    "        \n",
    "    if records:\n",
    "        df = pd.DataFrame(records) \n",
    "        # Do something with the obtained page\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"1012-knowledge_training_normal-20180131\"\n",
    "es_type = \"knowledge_training_normal\"\n",
    "beginTime = \"2018-01-26T00:00:00.000+08:00\"\n",
    "endTime = \"2018-01-27T00:00:00.000+08:00\"\n",
    "size = 10000\n",
    "tag = \"isException\"\n",
    "logmsg = \"logBody\"\n",
    "source_include = [tag, logmsg]\n",
    "\n",
    "es_util = Elasticsearch_Util()\n",
    "train_data_normal = getMoreDataFromES(es_util, index=index, doc_type=es_type,\n",
    "                           beginTime=beginTime, endTime=endTime,\n",
    "                           size=size,\n",
    "                           source_include=source_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1723"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"1012-knowledge_training_error-20180131\"\n",
    "es_type = \"knowledge_training_error\"\n",
    "beginTime = \"2018-01-30T00:00:00.000+08:00\"\n",
    "endTime = \"2018-02-01T00:00:00.000+08:00\"\n",
    "fromnum = 0\n",
    "size = 10000\n",
    "#1085  804:281\n",
    "tag = \"isException\"\n",
    "logmsg = \"logBody\"\n",
    "source_include = [tag, logmsg]\n",
    "\n",
    "es_util = Elasticsearch_Util()\n",
    "train_data_error = getMoreDataFromES(es_util, index=index, doc_type=es_type,\n",
    "                           beginTime=beginTime, endTime=endTime,\n",
    "                           size=size,\n",
    "                           source_include=source_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames=[train_data_normal,train_data_error]\n",
    "train_data=pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2527"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>logBody</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>isException</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1723</td>\n",
       "      <td>1723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804</td>\n",
       "      <td>804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id  logBody\n",
       "isException               \n",
       "0            1723     1723\n",
       "1             804      804"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.groupby('isException').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data_array = train_data[logmsg]\n",
    "class_data_array = train_data[tag]\n",
    "print len(feature_data_array)\n",
    "print len(class_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stem.porter import *\n",
    "\n",
    "#训练数据关键词提取列表\n",
    "def tokenize(text):\n",
    "    f = open(r\"E:\\zxdf\\ml\\linkdata-log\\dataset\\testrecord\\keyword_logbody.txt\", \"a+\")\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens2=[]\n",
    "    for w in tokens:\n",
    "        if w[0]=='-':\n",
    "            tokens2.append(w[1:])\n",
    "        elif w[0]==\"'\":\n",
    "            tokens2.append(w[1:])    \n",
    "        else:\n",
    "            tokens2.append(w)\n",
    "    \n",
    "    stopword_tokens = [i for i in tokens2 if i not in string.punctuation]\n",
    "    stopword_tokens2 = [w for w in stopword_tokens if w not in stopwords.words('english')]\n",
    "    \n",
    "    stopwords_custom=[\"''\", \"``\",\"||\",\"'/\",\"'/'\",u\"'0x0\",\"'2\",\"'=\",\"'s\",'**','***','***.***','**constraint.unique_wt**','**failed','**method','--',\n",
    "                      '-1','-297991290629036654','-614','-6182496564283649260','-6787310117729693199','-8','-999999999',u'-e',\n",
    "                      '..','...','.\\\\xxx\\\\xxx.txt','/*','/**','/c','/p','0','1','==','b','c','e','x','===',\n",
    "                      'v'\n",
    "                     ]\n",
    "    #,'mapped'\n",
    "    stopword_tokens3 = [w for w in stopword_tokens2 if w not in stopwords_custom]    \n",
    "            \n",
    "    print >> f, \"%s\" %(stopword_tokens3) \n",
    "    f.close()\n",
    "    return stopword_tokens3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer(tokenizer=tokenize, stop_words='english', decode_error='ignore')\n",
    "csr_mat = vectorizer.fit_transform(feature_data_array)\n",
    "# 获取词袋模型中的所有词\n",
    "wordlist = vectorizer.get_feature_names()  \n",
    "# tf矩阵 元素a[i][j]表示j词在i类文本中的tf\n",
    "countlist = csr_mat.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#countlist[:,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#feature_datas.toarray()\n",
    "#class_data_array\n",
    "wordcount_iserror_map = pd.DataFrame(data=np.c_[countlist, class_data_array],\n",
    "                     columns=np.append(wordlist, ['isException']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcount_iserror_map2=wordcount_iserror_map.apply(pd.to_numeric, errors='ignore')\n",
    "#['mapped'].astype(int)\n",
    "#.dtypes\n",
    "#.info()\n",
    "#.ix[:,20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wordcount_iserror_map2.dtypes\n",
    "#for x in \n",
    "print wordcount_iserror_map2.isnull().sum()\n",
    "#    print x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_word_in_doc(wordcount_iserror_map2,keyword):\n",
    "    errornum=0\n",
    "    normalnum=0\n",
    "    for (k1),group in wordcount_iserror_map2.ix[:,[keyword,'isException']].groupby('isException'):\n",
    "        print '%s' % '=========len========================='        \n",
    "        print '%s' % k1,':',len(group)\n",
    "        #print (group)\n",
    "        if k1==0:\n",
    "            for i in group.index:\n",
    "                if group[keyword][i]>0.0:\n",
    "                           normalnum += 1\n",
    "        elif k1==1:\n",
    "            for j in group.index:\n",
    "                if group[keyword][j]> 0.0: \n",
    "                    errornum += 1\n",
    "        else:\n",
    "            print 'this is a error'\n",
    "    \n",
    "    print '%s' % '=========count_word_in_doc========================='        \n",
    "    print '%s' % '1:',errornum\n",
    "    print '%s' % '0:',normalnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_word_in_doc(wordcount_iserror_map2,'null')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取特征\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#tf_transformer=TfidfTransformer()\n",
    "#feature_datas= tf_transformer.fit_transform(csr_mat)\n",
    "#\n",
    "## 获取词袋模型中的所有词\n",
    "##tfidfwordlist =tf_transformer.get_feature_names()\n",
    "## tf-idf矩阵 元素a[i][j]表示j词在i类文本中的tf-idf权重\n",
    "#weighttlist = feature_datas.toarray()\n",
    "from app.lib import nltk_text_analyze_lib as textAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_transformer = TfidfVectorizer(tokenizer=textAnalyzer.tokenize, stop_words='english', decode_error='ignore')\n",
    "feature_datas = tf_transformer.fit_transform(feature_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_datas.toarray()[:,0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#feature_datas.toarray()\n",
    "#class_data_array\n",
    "word_iserror_map = pd.DataFrame(data=np.c_[feature_datas.toarray(), class_data_array],\n",
    "                     columns=np.append(tf_transformer.get_feature_names(), ['isException']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_iserror_map = pd.DataFrame(data=np.c_[feature_datas.toarray(), class_data_array],\n",
    "                     columns=np.append(tf_transformer.get_feature_names(), ['isException']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_iserror_map.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word_iserror_map.iloc[:].groupby('isException').sum()\n",
    "word_iserror_map.ix[:,['mapped','null','isException']].groupby('isException').sum()\n",
    "#.groupby('isException').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查数据是否有缺失\n",
    "#word_iserror_map.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 观察样本中按类别数量是否比较均衡\n",
    "word_iserror_map.groupby('isException').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing import LabelEncoder\n",
    "#\n",
    "#le = LabelEncoder()\n",
    "#train_labelValues = le.fit_transform(class_data_array)\n",
    "#\n",
    "##, random_state=4\n",
    "#X_train, X_test, y_train, y_test = train_test_split(feature_datas,train_labelValues,test_size=0.25)\n",
    "#\n",
    "##from sklearn.metrics import accuracy_score\n",
    "#from sklearn import metrics\n",
    "#\n",
    "#model = DecisionTreeClassifier()\n",
    "#model.fit(X_train, y_train)\n",
    "#\n",
    "#y_pred_on_test = model.predict(X_test)\n",
    "#\n",
    "#print(metrics.precision_score(y_test, y_pred_on_test, average=None))\n",
    "#print(metrics.recall_score(y_test, y_pred_on_test, average='macro'))\n",
    "#print(metrics.f1_score(y_test, y_pred_on_test, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#验证算法\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def compareAlgorithm(X_train,Y_train,num_folds=10, seed=7, scoring='accuracy'):\n",
    "    models = {}\n",
    "    models['KNN'] = KNeighborsClassifier()\n",
    "    models['CART'] = DecisionTreeClassifier()\n",
    "    #models['SVM'] = SVC()\n",
    "    #models['LR'] = LogisticRegression()\n",
    "    # models['LDA'] = LinearDiscriminantAnalysis()\n",
    "    # models['NB'] = GaussianNB()\n",
    "    # 评估算法\n",
    "    results = []\n",
    "    for key in models:\n",
    "        kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "        cv_results = cross_val_score(models[key], X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "        print('%s: %f (%f)' %(key, cv_results.mean(), cv_results.std()))\n",
    "    return models, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compareAlgorithm(X_train=feature_datas, Y_train=class_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_labelValues = le.fit_transform(class_data_array)\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(feature_datas, train_labelValues)\n",
    "\n",
    "# teststack6678_804\n",
    "#app/rules/modelspkl\n",
    "#D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\n",
    "joblib.dump(tf_transformer, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack6678_804\\knowledge_tf_transformer.pkl')\n",
    "joblib.dump(le, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack6678_804\\knowledge_labelencoder.pkl')\n",
    "joblib.dump(model, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack6678_804\\knowledge_cart.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = DecisionTreeClassifier()\n",
    "#model.fit(feature_datas, class_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testline='-Mapped java.lang.StringIndexOutOfBoundsException: String index out of range: 0 '\n",
    "\n",
    "test_datas = []\n",
    "featureValue = testline.lower().strip()\n",
    "test_datas.append(featureValue)\n",
    "#test_feature_datas = tf_transformer.transform(test_datas)\n",
    "test_vect_datas=vectorizer.transform(test_datas)\n",
    "test_feature_datas= tf_transformer.transform(test_vect_datas)\n",
    "model.predict(test_feature_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
