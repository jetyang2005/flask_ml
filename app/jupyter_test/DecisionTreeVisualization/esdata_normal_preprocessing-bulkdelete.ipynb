{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "import os       \n",
    "import pandas as pd\n",
    "import datetime\n",
    "os.chdir('D:/zxdf/Workspaces/PycharmProjects/flask_ml')\n",
    "from app import config, Elasticsearch_Util\n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "print (\"当前的日期和时间是 %s\" % starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def getDataFromES2(es_util, index, es_type, beginTime, endTime, fromnum, size, source_include):\n",
    "#    # 最大查询10000条数据\n",
    "#    query_data = {\n",
    "#        \"from\": fromnum,\n",
    "#        \"size\": size,\n",
    "#        \"query\": {\n",
    "#            \"bool\": {\n",
    "#                \"must\": [\n",
    "#                    {\n",
    "#                        \"range\": {\n",
    "#                            \"@timestamp\": {\n",
    "#                                \"from\": beginTime,\n",
    "#                                \"to\": endTime\n",
    "#                            }\n",
    "#                        }\n",
    "#                    }\n",
    "#                ],\n",
    "#            }\n",
    "#        },\n",
    "#        \"_source\": {\n",
    "#            \"includes\": source_include,\n",
    "#            \"excludes\": []\n",
    "#        },\n",
    "#        \"sort\": [\n",
    "#            {\n",
    "#                \"@timestamp\": {\n",
    "#                    \"order\": \"desc\"\n",
    "#                }\n",
    "#            }\n",
    "#        ]\n",
    "#    }\n",
    "#    res = es_util.query(index, es_type, query_data)\n",
    "#    df = None\n",
    "#    records = []\n",
    "#    for doc in res['hits']['hits']:\n",
    "#        source=doc['_source']\n",
    "#        source[\"id\"]=doc['_id']\n",
    "#        records.append(source)\n",
    "#    if records:\n",
    "#        df = pd.DataFrame(records)\n",
    "#    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def getDataFromES(es_util, index, es_type, beginTime, endTime, fromnum, size, source_include):\n",
    "#    # 最大查询10000条数据\n",
    "#    query_data = {\n",
    "#        \"from\": fromnum,\n",
    "#        \"size\": size,\n",
    "#        \"query\": {\n",
    "#            \"bool\": {\n",
    "#                \"must\": [\n",
    "#                    {\n",
    "#                        #\"match\":{\n",
    "#                        #    \"isException\":\"0\"\n",
    "#                        #}\n",
    "#                        \"range\": {\n",
    "#                            \"@timestamp\": {\n",
    "#                                \"from\": beginTime,\n",
    "#                                \"to\": endTime\n",
    "#                            }\n",
    "#                        }\n",
    "#                    }\n",
    "#                ],\n",
    "#            }\n",
    "#        },\n",
    "#        \"_source\": {\n",
    "#            \"includes\": source_include,\n",
    "#            \"excludes\": []\n",
    "#        },\n",
    "#        \"sort\": [\n",
    "#            {\n",
    "#                \"@timestamp\": {\n",
    "#                    \"order\": \"desc\"\n",
    "#                }\n",
    "#            }\n",
    "#        ]\n",
    "#    }\n",
    "#\n",
    "#    res = es_util.query(index, es_type, query_data)\n",
    "#    df = None\n",
    "#    records = []\n",
    "#    for doc in res['hits']['hits']:\n",
    "#        source=doc['_source']\n",
    "#        source[\"id\"]=doc['_id']\n",
    "#        source[\"index\"]=doc['_index']  \n",
    "#        records.append(source)\n",
    "#    if records:\n",
    "#        df = pd.DataFrame(records)\n",
    "#    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMoreDataFromES(self, index, doc_type,beginTime, endTime, source_include,size):\n",
    "    querybody = {\n",
    "        \"query\": {\n",
    "            \"match_all\": {}\n",
    "        },\n",
    "        \"_source\": {\n",
    "            \"includes\": source_include,\n",
    "            \"excludes\": []\n",
    "        }\n",
    "    }\n",
    "    # Initialize the scroll\n",
    "    print \"Initialize the scroll\"\n",
    "    page = self.es.search(\n",
    "        index=index,\n",
    "        doc_type=doc_type,\n",
    "        scroll='2m',\n",
    "        size=size,\n",
    "        body=querybody)\n",
    "    sid = page['_scroll_id']\n",
    "    scroll_size = page['hits']['total']\n",
    "    \n",
    "    print \"sid: \" + str(sid)\n",
    "    print \"scroll size: \" + str(scroll_size)\n",
    "    \n",
    "    records = []\n",
    "    for doc in page['hits']['hits']:\n",
    "            source=doc['_source']\n",
    "            source[\"id\"]=doc['_id']\n",
    "            source[\"index\"]=doc['_index'] \n",
    "#            if doc['_source']['logBody']!=None and doc['_source']['logBody']!=\"\":\n",
    "            records.append(source)\n",
    "    \n",
    "    df = None\n",
    "   #Start scrolling\n",
    "    while (scroll_size > 0):\n",
    "        print \"Scrolling...\"\n",
    "        page = self.es.scroll(scroll_id=sid, scroll='2m')\n",
    "        # Update the scroll ID\n",
    "        sid = page['_scroll_id']\n",
    "        print \"sid: \" + str(sid)\n",
    "        # Get the number of results that we returned in the last scroll\n",
    "        scroll_size = len(page['hits']['hits'])\n",
    "        \n",
    "        for doc in page['hits']['hits']:\n",
    "            source=doc['_source']\n",
    "            source[\"id\"]=doc['_id']\n",
    "            source[\"index\"]=doc['_index']\n",
    "#            if doc['_source']['logBody']!=None and doc['_source']['logBody']!=\"\":\n",
    "            records.append(source)\n",
    "            \n",
    "        print \"scroll size: \" + str(scroll_size)\n",
    "        \n",
    "    if records:\n",
    "        df = pd.DataFrame(records) \n",
    "        # Do something with the obtained page\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index = \"1012-knowledge_training_normal-*\"\n",
    "index = \"1012-knowledge_training_normal-20180206\"\n",
    "es_type = \"knowledge_training_normal\"\n",
    "beginTime = \"2018-02-05T00:00:00.000+08:00\"\n",
    "endTime = \"2018-02-06T00:00:00.000+08:00\"\n",
    "size = 10000\n",
    "#1085  804:281\n",
    "tag = \"isException\"\n",
    "logmsg = \"logBody\"\n",
    "source_include = [tag, logmsg]\n",
    "\n",
    "es_util = Elasticsearch_Util()\n",
    "train_data = getMoreDataFromES(es_util, index=index, doc_type=es_type,\n",
    "                           beginTime=beginTime, endTime=endTime,\n",
    "                           size=size,\n",
    "                           source_include=source_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index = \"1012-knowledge_training-*\"\n",
    "#es_type = \"knowledge_training\"\n",
    "#index = \"1012-knowledge_training_normal-20180206\"\n",
    "#es_type = \"knowledge_training_normal\"\n",
    "#beginTime = \"2018-02-06T00:00:00.000+08:00\"\n",
    "#endTime = \"2018-02-07T00:00:00.000+08:00\"\n",
    "#fromnum = 0\n",
    "#size = 5000\n",
    "##1085  804:281\n",
    "#tag = \"isException\"\n",
    "#logmsg = \"logBody\"\n",
    "#source_include = [tag, logmsg]\n",
    "#\n",
    "#es_util = Elasticsearch_Util()\n",
    "#train_data = getDataFromES(es_util, index=index, es_type=es_type,\n",
    "#                           beginTime=beginTime, endTime=endTime,\n",
    "#                           fromnum=fromnum, size=size,\n",
    "#                           source_include=source_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data_array = train_data[logmsg]\n",
    "class_data_array = train_data[tag]\n",
    "print len(feature_data_array)\n",
    "print len(class_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import codecs\n",
    "#训练数据日志体\n",
    "#f = codecs.open(r\"E:\\zxdf\\ml\\linkdata-log\\dataset\\testrecord\\logbody.txt\", \"w\", encoding='unicode')\n",
    "f = open(r\"E:\\zxdf\\ml\\linkdata-log\\dataset\\testrecord\\logbody.txt\", \"w\")\n",
    "for indexs in train_data.index:\n",
    "    print >> f, \"%s%s%s%s\" %(train_data.loc[indexs].values[0:1],train_data.loc[indexs].values[1:2],train_data.loc[indexs].values[2:3],train_data.loc[indexs].values[3:4])\n",
    "    #f.write(content)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#训练数据关键词提取列表\n",
    "def tokenize(text):\n",
    "    f = open(r\"E:\\zxdf\\ml\\linkdata-log\\dataset\\testrecord\\keyword_logbody.txt\", \"a+\")\n",
    "    #stopword_tokens3 =[]\n",
    "    \n",
    "    #if text !=None:\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens2=[]\n",
    "    for w in tokens:\n",
    "        if w[0]=='-':\n",
    "            tokens2.append(w[1:])\n",
    "        elif w[0]==\"'\":\n",
    "            tokens2.append(w[1:])    \n",
    "        else:\n",
    "            tokens2.append(w)\n",
    "    \n",
    "    stopword_tokens = [i for i in tokens2 if i not in string.punctuation]\n",
    "    stopword_tokens2 = [w for w in stopword_tokens if w not in stopwords.words('english')]\n",
    "    \n",
    "    stopwords_custom=[\"''\", \"``\",\"||\",\"'/\",\"'/'\",u\"'0x0\",\"'2\",\"'=\",\"'s\",'**','***','***.***','**constraint.unique_wt**','**failed','**method','--',\n",
    "                      '-1','-297991290629036654','-614','-6182496564283649260','-6787310117729693199','-8','-999999999',u'-e',\n",
    "                      '..','...','.\\\\xxx\\\\xxx.txt','/*','/**','/c','/p','0','1','==','b','c','e','x','===',\n",
    "                      'v'\n",
    "                     ]\n",
    "    stopword_tokens3 = [w for w in stopword_tokens2 if w not in stopwords_custom]    \n",
    "            \n",
    "    print >> f, \"%s\" %(stopword_tokens3)\n",
    "    f.close()\n",
    "    return stopword_tokens3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer=CountVectorizer(tokenizer=tokenize, stop_words='english', decode_error='ignore')\n",
    "csr_mat = vectorizer.fit_transform(feature_data_array)\n",
    "# 获取词袋模型中的所有词\n",
    "wordlist = vectorizer.get_feature_names()  \n",
    "# tf矩阵 元素a[i][j]表示j词在i类文本中的tf\n",
    "countlist = csr_mat.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获得词频统计列表（包含该词的文档数，词，所有出现改词的文档列表[(文档index,文档id)]）\n",
    "def wordcount(wordlist,countlist,train_data):\n",
    "    wclist=[]\n",
    "    for j in range(len(wordlist)):    \n",
    "        esidarray=[]\n",
    "        for i in range(len(countlist)):\n",
    "            if countlist[i][j]>0:\n",
    "                esidarray.append((train_data[\"index\"][i],train_data[\"id\"][i]))\n",
    "        #（包含该词的文档数，词，所有出现改词的文档列表[文档id]）        \n",
    "        textcontent=(len(esidarray),wordlist[j],esidarray)       \n",
    "        wclist.append(textcontent)\n",
    "    return  wclist   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wclist=wordcount(wordlist,countlist,train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获得倒序的词频统计列表\n",
    "f = open(r\"E:\\zxdf\\ml\\linkdata-log\\dataset\\testrecord\\tf_tfidfs.txt\", \"w\")\n",
    "for text in sorted(wclist, reverse=True):\n",
    "    print >> f, text\n",
    "f.close()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取要删除的高频词所对应的文档id列表\n",
    "def delword(wclist,delwordnum):\n",
    "    dellist=[]\n",
    "    for i in range(len(wclist)):\n",
    "         if wclist[i][0] >= delwordnum:\n",
    "                dellist.append(wclist[i])\n",
    "    return dellist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获得超过指定词频的列表（包含该词的文档数，词，所有出现改词的文档列表[文档id]）   \n",
    "delwordnum=20\n",
    "#2\n",
    "#5\n",
    "#10\n",
    "#20\n",
    "#50\n",
    "#17954\n",
    "#10028\n",
    "#17635\n",
    "#17954\n",
    "#9024\n",
    "dellist=delword(wclist,delwordnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dellist[0][2][0]\n",
    "len(dellist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def getDataFromESbyid(esid):\n",
    "#    # 最大查询10000条数据\n",
    "#    query_data ={\n",
    "#        \"query\":{\n",
    "#            \"bool\":{\n",
    "#                \"must\":[\n",
    "#                    {\n",
    "#                        \"match\":{\n",
    "#                            \"_id\":esid\n",
    "#                        }\n",
    "#                    }\n",
    "#                ]\n",
    "#            }\n",
    "#        }\n",
    "#    }\n",
    "#    return query_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([config.ES_HOST],\n",
    "                   http_auth=('admin', 'admin'),\n",
    "                   port=config.ES_PORT)\n",
    "#esid=\"AWEwSCqvLBpMZiso47WP\"\n",
    "#res = es_util.query(index, es_type, getDataFromESbyid(esid)) \n",
    "#index = res[\"hits\"][\"hits\"][0][\"_index\"]\n",
    "\n",
    "#index = \"1012-knowledge_training-20180130\"\n",
    "#es_type = \"knowledge_training\"\n",
    "\n",
    "#index = \"1012-knowledge_training_normal-20180131\"\n",
    "#es_type = \"knowledge_training_normal\"\n",
    "\n",
    "#res=es.exists(index=index, doc_type=es_type, id=esid)\n",
    "#resget = es.get_source(index=index, doc_type=es_type, id=esid)\n",
    "#res2 = es.delete(index, doc_type=es_type, id=esid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dellist[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch.helpers import bulk \n",
    "#from elasticsearch.helpers import parallel_bulk\n",
    "sys.setdefaultencoding('utf8')\n",
    "\n",
    "def delid(es, dellist,delidnum, index_name, doc_type_name):\n",
    "    i = 0\n",
    "    count = 0\n",
    "    ACTIONS = []\n",
    "    for i in range(len(dellist)):\n",
    "        if len(dellist[i])==3:\n",
    "            delesids= dellist[i][2]\n",
    "            #print '%s' % '=====================',dellist[i][1],'========================='\n",
    "            for j in range(len(delesids)):\n",
    "                if j>= delidnum:\n",
    "                    #if not delesids[j] in everdellist: \n",
    "                        #print delesids[j][0],delesids[j][1]\n",
    "                        #result =es.delete(delesids[j][0], doc_type=es_type, id=delesids[j][1])\n",
    "                    action = {\n",
    "                        \"_op_type\": \"delete\",\n",
    "                        \"_index\": delesids[j][0],\n",
    "                        \"_type\": doc_type_name,\n",
    "                        \"_id\": delesids[j][1]\n",
    "                    }\n",
    "                    i += 1\n",
    "                    ACTIONS.append(action)\n",
    "                    if i == 50000:\n",
    "                        #success, _ = \n",
    "                        bulk(es, ACTIONS, raise_on_error = True)\n",
    "                        #count += success\n",
    "                        i = 0\n",
    "                        ACTIONS = []\n",
    "    \n",
    "    #success, _ = \n",
    "    bulk(es, ACTIONS, raise_on_error = True)\n",
    "    #count += success\n",
    "    #print(\"del %s lines\" % count)\n",
    "\n",
    "#获删除高频词所对应的文档id列表\n",
    "#def delid(index,es_type,dellist,delidnum):\n",
    "#    for i in range(len(dellist)):\n",
    "#        delesids= dellist[i][2]\n",
    "#        #print '%s' % '=====================',dellist[i][1],'========================='\n",
    "#        for j in range(len(delesids)):\n",
    "#            if j>= delidnum:\n",
    "#                if not delesids[j] in everdellist: \n",
    "#                    #print delesids[j][0],delesids[j][1]\n",
    "#                    result =es.delete(delesids[j][0], doc_type=es_type, id=delesids[j][1])\n",
    "#                    if result[\"result\"]==\"deleted\":\n",
    "#                        everdellist.append(delesids[j])\n",
    "#                        print \"%s\" % 'success delid:',delesids[j],';result:',result\n",
    "#                    else:\n",
    "#                        print \"%s\" % 'faile delid:',delesids[j],';result:',result\n",
    "#                \n",
    "                \n",
    "                \n",
    "#{u'_type': u'knowledge_training_normal', u'_index': u'1012-knowledge_training_normal-20180131', u'_shards': {u'successful': 1, u'failed': 0, u'total': 2}, u'_version\n",
    "#': 2, u'result': u'deleted', u'found': True, u'_id': u'AWFKeRaILBpMZisoN8iF'}                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delidnum=20\n",
    "delid(es,dellist,delidnum,index,es_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
