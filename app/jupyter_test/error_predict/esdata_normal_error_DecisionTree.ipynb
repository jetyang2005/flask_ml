{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import sys\n",
    "import os       \n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "os.chdir('D:/zxdf/Workspaces/PycharmProjects/flask_ml')\n",
    "from app import config, Elasticsearch_Util\n",
    "#from app.lib import nltk_text_analyze_lib as textAnalyzer\n",
    "#from app.lib import sklearn_classification_lib as skclassification\n",
    "\n",
    "starttime = datetime.datetime.now()\n",
    "print (\"当前的日期和时间是 %s\" % starttime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMoreDataFromES(self, index, doc_type,beginTime, endTime, source_include,size):\n",
    "    querybody = {\n",
    "        \"query\": {\n",
    "            \"match_all\": {}\n",
    "        },\n",
    "        \"_source\": {\n",
    "            \"includes\": source_include,\n",
    "            \"excludes\": []\n",
    "        }\n",
    "    }\n",
    "    # Initialize the scroll\n",
    "    print \"Initialize the scroll\"\n",
    "    page = self.es.search(\n",
    "        index=index,\n",
    "        doc_type=doc_type,\n",
    "        scroll='2m',\n",
    "        size=size,\n",
    "        body=querybody)\n",
    "    sid = page['_scroll_id']\n",
    "    scroll_size = page['hits']['total']\n",
    "    \n",
    "    print \"sid: \" + str(sid)\n",
    "    print \"scroll size: \" + str(scroll_size)\n",
    "    \n",
    "    records = []\n",
    "    for doc in page['hits']['hits']:\n",
    "            source=doc['_source']\n",
    "            source[\"id\"]=doc['_id']\n",
    "            records.append(source)\n",
    "    \n",
    "    df = None\n",
    "   #Start scrolling\n",
    "    while (scroll_size > 0):\n",
    "        print \"Scrolling...\"\n",
    "        page = self.es.scroll(scroll_id=sid, scroll='2m')\n",
    "        # Update the scroll ID\n",
    "        sid = page['_scroll_id']\n",
    "        print \"sid: \" + str(sid)\n",
    "        # Get the number of results that we returned in the last scroll\n",
    "        scroll_size = len(page['hits']['hits'])\n",
    "        \n",
    "        for doc in page['hits']['hits']:\n",
    "            source=doc['_source']\n",
    "            source[\"id\"]=doc['_id']\n",
    "            records.append(source)\n",
    "            \n",
    "        print \"scroll size: \" + str(scroll_size)\n",
    "        \n",
    "    if records:\n",
    "        df = pd.DataFrame(records) \n",
    "        # Do something with the obtained page\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"1012-knowledge_training_normal-*\"\n",
    "es_type = \"knowledge_training_normal\"\n",
    "beginTime = \"2018-01-26T00:00:00.000+08:00\"\n",
    "endTime = \"2018-01-27T00:00:00.000+08:00\"\n",
    "size = 10000\n",
    "tag = \"isException\"\n",
    "logmsg = \"logBody\"\n",
    "source_include = [tag, logmsg]\n",
    "\n",
    "es_util = Elasticsearch_Util()\n",
    "train_data_normal = getMoreDataFromES(es_util, index=index, doc_type=es_type,\n",
    "                           beginTime=beginTime, endTime=endTime,\n",
    "                           size=size,\n",
    "                           source_include=source_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "692"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = \"1012-knowledge_training_error-*\"\n",
    "es_type = \"knowledge_training_error\"\n",
    "beginTime = \"2018-01-30T00:00:00.000+08:00\"\n",
    "endTime = \"2018-02-01T00:00:00.000+08:00\"\n",
    "fromnum = 0\n",
    "size = 10000\n",
    "#1085  804:281\n",
    "tag = \"isException\"\n",
    "logmsg = \"logBody\"\n",
    "source_include = [tag, logmsg]\n",
    "\n",
    "es_util = Elasticsearch_Util()\n",
    "train_data_error = getMoreDataFromES(es_util, index=index, doc_type=es_type,\n",
    "                           beginTime=beginTime, endTime=endTime,\n",
    "                           size=size,\n",
    "                           source_include=source_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "804"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames=[train_data_normal,train_data_error]\n",
    "train_data=pd.concat(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1496"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_data_array = train_data[logmsg]\n",
    "class_data_array = train_data[tag]\n",
    "print len(feature_data_array)\n",
    "print len(class_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "#from nltk.stem.porter import *\n",
    "\n",
    "#训练数据关键词提取列表\n",
    "#def tokenize(text):\n",
    "#    f = open(r\"E:\\zxdf\\ml\\linkdata-log\\dataset\\testrecord\\keyword_logbody.txt\", \"a+\")\n",
    "#    tokens = nltk.word_tokenize(text)\n",
    "#    tokens2=[]\n",
    "#    for w in tokens:\n",
    "#        if w[0]=='-':\n",
    "#            tokens2.append(w[1:])\n",
    "#        elif w[0]==\"'\":\n",
    "#            tokens2.append(w[1:])    \n",
    "#        else:\n",
    "#            tokens2.append(w)\n",
    "#    \n",
    "#    stopword_tokens = [i for i in tokens2 if i not in string.punctuation]\n",
    "#    stopword_tokens2 = [w for w in stopword_tokens if w not in stopwords.words('english')]\n",
    "#    \n",
    "#    stopwords_custom=[\"''\", \"``\",\"||\",\"'/\",\"'/'\",u\"'0x0\",\"'2\",\"'=\",\"'s\",'**','***','***.***','**constraint.unique_wt**','**failed','**method','--',\n",
    "#                      '-1','-297991290629036654','-614','-6182496564283649260','-6787310117729693199','-8','-999999999',u'-e',\n",
    "#                      '..','...','.\\\\xxx\\\\xxx.txt','/*','/**','/c','/p','0','1','==','b','c','e','x','===',\n",
    "#                      'v'\n",
    "#                     ]\n",
    "#    #,'mapped'\n",
    "#    stopword_tokens3 = [w for w in stopword_tokens2 if w not in stopwords_custom]    \n",
    "#            \n",
    "#    print >> f, \"%s\" %(stopword_tokens3) \n",
    "#    f.close()\n",
    "#    return stopword_tokens3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.feature_extraction.text import CountVectorizer\n",
    "#vectorizer=CountVectorizer(tokenizer=tokenize, stop_words='english', decode_error='ignore')\n",
    "#csr_mat = vectorizer.fit_transform(feature_data_array)\n",
    "## 获取词袋模型中的所有词\n",
    "#wordlist = vectorizer.get_feature_names()  \n",
    "## tf矩阵 元素a[i][j]表示j词在i类文本中的tf\n",
    "#countlist = csr_mat.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提取特征\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#tf_transformer=TfidfTransformer()\n",
    "#feature_datas= tf_transformer.fit_transform(csr_mat)\n",
    "#\n",
    "## 获取词袋模型中的所有词\n",
    "##tfidfwordlist =tf_transformer.get_feature_names()\n",
    "## tf-idf矩阵 元素a[i][j]表示j词在i类文本中的tf-idf权重\n",
    "#weighttlist = feature_datas.toarray()\n",
    "from app.lib import nltk_text_analyze_lib as textAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_transformer = TfidfVectorizer(tokenizer=textAnalyzer.tokenize, stop_words='english', decode_error='ignore')\n",
    "feature_datas = tf_transformer.fit_transform(feature_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature_datas.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#验证算法\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#from sklearn.svm import SVC\n",
    "#from sklearn.linear_model import LogisticRegression\n",
    "#from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "#from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def compareAlgorithm(X_train,Y_train,num_folds=10, seed=7, scoring='accuracy'):\n",
    "    models = {}\n",
    "    models['KNN'] = KNeighborsClassifier()\n",
    "    models['CART'] = DecisionTreeClassifier()\n",
    "    #models['SVM'] = SVC()\n",
    "    #models['LR'] = LogisticRegression()\n",
    "    #models['LDA'] = LinearDiscriminantAnalysis()\n",
    "    #models['NB'] = GaussianNB()\n",
    "    # 评估算法\n",
    "    results = []\n",
    "    for key in models:\n",
    "        kfold = KFold(n_splits=num_folds, random_state=seed)\n",
    "        cv_results = cross_val_score(models[key], X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "        results.append(cv_results)\n",
    "        print('%s: %f (%f)' %(key, cv_results.mean(), cv_results.std()))\n",
    "    return models, results\n",
    "#标准偏差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'CART': DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "              splitter='best'),\n",
       "  'KNN': KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "             metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "             weights='uniform')},\n",
       " [array([ 1.        ,  1.        ,  1.        ,  1.        ,  0.61333333,\n",
       "          0.04666667,  0.03355705,  0.02684564,  0.02013423,  0.05369128]),\n",
       "  array([ 0.97333333,  0.99333333,  0.96666667,  1.        ,  0.84666667,\n",
       "          0.42      ,  0.43624161,  0.48993289,  0.42281879,  0.48993289])])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compareAlgorithm(X_train=feature_datas, Y_train=class_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_labelValues = le.fit_transform(class_data_array)\n",
    "\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(feature_datas, train_labelValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# teststack6678_804\n",
    "#app/rules/modelspkl\n",
    "#D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\n",
    "#joblib.dump(tf_transformer, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack6678_804\\knowledge_tf_transformer.pkl')\n",
    "#joblib.dump(le, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack6678_804\\knowledge_labelencoder.pkl')\n",
    "#joblib.dump(model, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack6678_804\\knowledge_cart.pkl')\n",
    "\n",
    "\n",
    "# teststack2527_804\n",
    "#app/rules/modelspkl\n",
    "#D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\n",
    "#joblib.dump(tf_transformer, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack2527_804\\knowledge_tf_transformer.pkl')\n",
    "#joblib.dump(le, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack2527_804\\knowledge_labelencoder.pkl')\n",
    "#joblib.dump(model, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack2527_804\\knowledge_cart.pkl')\n",
    "\n",
    "# teststack1662_804\n",
    "#joblib.dump(tf_transformer, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack1662_804\\knowledge_tf_transformer.pkl')\n",
    "#joblib.dump(le, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack1662_804\\knowledge_labelencoder.pkl')\n",
    "#joblib.dump(model, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack1662_804\\knowledge_cart.pkl')\n",
    "\n",
    "\n",
    "# teststack489_804\n",
    "#joblib.dump(tf_transformer, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack489_804\\knowledge_tf_transformer.pkl')\n",
    "#joblib.dump(le, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack489_804\\knowledge_labelencoder.pkl')\n",
    "#joblib.dump(model, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack489_804\\knowledge_cart.pkl')\n",
    "\n",
    "# teststack290_804\n",
    "#joblib.dump(tf_transformer, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack290_804\\knowledge_tf_transformer.pkl')\n",
    "#joblib.dump(le, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack290_804\\knowledge_labelencoder.pkl')\n",
    "#joblib.dump(model, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack290_804\\knowledge_cart.pkl')\n",
    "\n",
    "# teststack150_804\n",
    "#joblib.dump(tf_transformer, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack150_804\\knowledge_tf_transformer.pkl')\n",
    "#joblib.dump(le, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack150_804\\knowledge_labelencoder.pkl')\n",
    "#joblib.dump(model, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\teststack150_804\\knowledge_cart.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normal_preprocessing01\\teststack22371_804\n",
    "#joblib.dump(tf_transformer, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\normal_preprocessing01\\teststack22371_804\\knowledge_tf_transformer.pkl')\n",
    "#joblib.dump(le, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\normal_preprocessing01\\teststack22371_804\\knowledge_labelencoder.pkl')\n",
    "#joblib.dump(model, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\normal_preprocessing01\\teststack22371_804\\knowledge_cart.pkl')\n",
    "\n",
    "\n",
    "# normal_preprocessing01\\teststack773_804\n",
    "#joblib.dump(tf_transformer, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\normal_preprocessing01\\teststack773_804\\knowledge_tf_transformer.pkl')\n",
    "#joblib.dump(le, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\normal_preprocessing01\\teststack773_804\\knowledge_labelencoder.pkl')\n",
    "#joblib.dump(model, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\normal_preprocessing01\\teststack773_804\\knowledge_cart.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['D:\\\\zxdf\\\\Workspaces\\\\PycharmProjects\\\\flask_ml\\\\app\\\\rules\\\\modelspkl\\\\normal_preprocessing02\\\\teststack692_804\\\\knowledge_cart.pkl']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normal_preprocessing02\\teststack1618_804\n",
    "#joblib.dump(tf_transformer, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\normal_preprocessing02\\teststack1618_804\\knowledge_tf_transformer.pkl')\n",
    "#joblib.dump(le, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\normal_preprocessing02\\teststack1618_804\\knowledge_labelencoder.pkl')\n",
    "#joblib.dump(model, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\normal_preprocessing02\\teststack1618_804\\knowledge_cart.pkl')\n",
    "\n",
    "# normal_preprocessing02\\teststack1043_804\n",
    "#joblib.dump(tf_transformer, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\normal_preprocessing02\\teststack1043_804\\knowledge_tf_transformer.pkl')\n",
    "#joblib.dump(le, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\normal_preprocessing02\\teststack1043_804\\knowledge_labelencoder.pkl')\n",
    "#joblib.dump(model, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\normal_preprocessing02\\teststack1043_804\\knowledge_cart.pkl')\n",
    "\n",
    "# normal_preprocessing02\\teststack692_804\n",
    "joblib.dump(tf_transformer, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\normal_preprocessing02\\teststack692_804\\knowledge_tf_transformer.pkl')\n",
    "joblib.dump(le, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\normal_preprocessing02\\teststack692_804\\knowledge_labelencoder.pkl')\n",
    "joblib.dump(model, r'D:\\zxdf\\Workspaces\\PycharmProjects\\flask_ml\\app\\rules\\modelspkl\\normal_preprocessing02\\teststack692_804\\knowledge_cart.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = DecisionTreeClassifier()\n",
    "#model.fit(feature_datas, class_data_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testline='-Mapped java.lang.StringIndexOutOfBoundsException: String index out of range: 0 '\n",
    "#\n",
    "#test_datas = []\n",
    "#featureValue = testline.lower().strip()\n",
    "#test_datas.append(featureValue)\n",
    "##test_feature_datas = tf_transformer.transform(test_datas)\n",
    "#test_vect_datas=vectorizer.transform(test_datas)\n",
    "#test_feature_datas= tf_transformer.transform(test_vect_datas)\n",
    "#model.predict(test_feature_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
